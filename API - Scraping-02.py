# -*- coding: utf-8 -*-
"""M32.05 - Assignment V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kEv7dHP4-wYAb3fDLRT5avn0ymxhTX-O
"""

from time import time
from time import sleep
from random import randint
from IPython.core.display import clear_output
from warnings import warn 
from bs4 import BeautifulSoup
import requests

#You should submit a notebook that contains a function that can be run that 
#retrieves the space.com news page, extracts the news stories, and prints out 
#the headline, author, synopsis, and date and time for each story

  # url = 'https://www.space.com/news'
  # response = requests.get(url)

  # if (response.ok):
  #   data = response.text

  # soup = BeautifulSoup(data,'html.parser')

  # soup.find_all('div class=content')

def process_page(soup, stories):

  # list = soup.find('ol')
  # items = list.find_all('li')
  # stories = [story.get_text() for story in items]
  # print(stories)


  # div = soup.find('div', class_='content')
  # list = div.find('header')
  # items = list.find_all('h3')
  # stories = [story.get_text() for story in items]

  news_stories = soup.select('.content')

  stories = []
  for story in news_stories:
    headline = story.select_one('.article-name').get_text().strip()
    author = story.select_one('.by-author').get_text().strip()
    synopsis = story.select_one('.synopsis').get_text().strip()
    date_time = story.select_one('.published-date relative-date')
    new_stories = {'article-name':headline, 'author':author, 'synopsis':synopsis, "published-date relative-date":date_time,}
    stories.append(news_stories)

start_time = time()
request_count = 0

store_stories = []

has_next_page = True
MAX_REQUESTS = 10 
page_number = 1
query = {'tab':'newest', 'page':page_number}
url = 'https://www.space.com/news'
headers = {'user-agent': 'storyscraper - school project (jojonomnoms.eats@gmail.com)'}

while has_next_page and request_count < MAX_REQUESTS:

  clear_output(wait = True)
  

  response = requests.get(url, params=query, headers=headers)


  if(response.ok):

    data = response.text
    soup = BeautifulSoup(data, 'html.parser')
    process_page(soup, stories)

    next_button = soup.select('a[rel="next"]')
    has_next_page = len(next_button) > 0

  else:
    warn('Request #: {}, Failed with status code: {}'.format(request_count, response.status_code))

  request_count += 1

  sleep(randint(1, 3))

  elapsed_time = time() - start_time
  print('Requests: {}, Frequency: {} requests/s, {} stories processed.'.format(request_count, request_count/elapsed_time, len(stories)))

  page_number += 1 

print('Sraping complete')
print('Requests: {}, Frequency: {} requests/s, {} stories processed.'.format(request_count, request_count/elapsed_time, len(stories)))

print('-------LINE BREAK-------')
print(stories)